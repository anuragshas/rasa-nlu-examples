import os
from typing import Any, Dict, List, Text

import sentencepiece as spm

from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer
from rasa.nlu.training_data import Message


class SentencePieceTokenizer(Tokenizer):

    defaults = {
        # Flag to check whether to split intents
        "intent_tokenization_flag": False,
        # Symbol on which intent should be split
        "intent_split_symbol": "_",
        # Text will be tokenized with case sensitive as default
        "case_sensitive": True,
        # What language to use
        "lang": None,
        # specifies the path to a custom SentencePiece model file
        "model_file": None,
    }

    supported_language_list = [
        "mt",
        "sd",
        "cr",
        "ba",
        "ht",
        "scn",
        "bi",
        "stq",
        "sm",
        "diq",
        "no",
        "yi",
        "vec",
        "bug",
        "am",
        "tl",
        "mn",
        "atj",
        "ko",
        "mai",
        "lij",
        "tcy",
        "sl",
        "bn",
        "dv",
        "rm",
        "ng",
        "ml",
        "kg",
        "koi",
        "war",
        "et",
        "mhr",
        "als",
        "bar",
        "ii",
        "sco",
        "got",
        "pnb",
        "ss",
        "bpy",
        "tum",
        "ru",
        "qu",
        "hy",
        "tw",
        "bm",
        "vep",
        "dty",
        "udm",
        "gd",
        "lbe",
        "rmy",
        "azb",
        "kw",
        "ja",
        "wuu",
        "pag",
        "ro",
        "tet",
        "ee",
        "min",
        "su",
        "ha",
        "glk",
        "pcd",
        "tk",
        "nrm",
        "ku",
        "gn",
        "ty",
        "bh",
        "pap",
        "fr",
        "ia",
        "cs",
        "ky",
        "ff",
        "kab",
        "rn",
        "csb",
        "tt",
        "cy",
        "ilo",
        "kaa",
        "hif",
        "ak",
        "pa",
        "crh",
        "ti",
        "myv",
        "ur",
        "se",
        "uz",
        "cdo",
        "lez",
        "srn",
        "kk",
        "pih",
        "de",
        "an",
        "tyv",
        "ext",
        "gan",
        "wo",
        "si",
        "lmo",
        "hak",
        "az",
        "ka",
        "ik",
        "frr",
        "hsb",
        "ho",
        "af",
        "nds",
        "pam",
        "el",
        "fur",
        "cu",
        "hr",
        "my",
        "nl",
        "da",
        "ch",
        "vls",
        "es",
        "as",
        "lt",
        "ny",
        "so",
        "oc",
        "lad",
        "pnt",
        "ms",
        "bcl",
        "os",
        "co",
        "ks",
        "or",
        "ay",
        "wa",
        "nah",
        "fa",
        "pl",
        "mzn",
        "za",
        "th",
        "fj",
        "kbp",
        "be",
        "zh",
        "ce",
        "sh",
        "sr",
        "id",
        "chy",
        "ps",
        "lo",
        "tr",
        "st",
        "he",
        "ang",
        "sah",
        "io",
        "gom",
        "ki",
        "sn",
        "kbd",
        "jam",
        "bo",
        "pms",
        "sk",
        "kv",
        "ckb",
        "nv",
        "dsb",
        "zea",
        "xmf",
        "fi",
        "ltg",
        "ksh",
        "ve",
        "new",
        "na",
        "jv",
        "tn",
        "sw",
        "rw",
        "ln",
        "bs",
        "gag",
        "ab",
        "olo",
        "is",
        "bjn",
        "ceb",
        "om",
        "vi",
        "ast",
        "uk",
        "mg",
        "mwl",
        "arz",
        "li",
        "mrj",
        "yo",
        "frp",
        "gl",
        "la",
        "km",
        "sv",
        "nap",
        "jbo",
        "bxr",
        "gv",
        "br",
        "fo",
        "ug",
        "pi",
        "bg",
        "ie",
        "din",
        "sa",
        "pdc",
        "cho",
        "lb",
        "ig",
        "aa",
        "sc",
        "fy",
        "kj",
        "eo",
        "eu",
        "kl",
        "sq",
        "to",
        "mi",
        "tpi",
        "kr",
        "hi",
        "arc",
        "ga",
        "nov",
        "mdf",
        "vo",
        "pfl",
        "rue",
        "haw",
        "kn",
        "mh",
        "mr",
        "te",
        "ca",
        "ace",
        "cv",
        "zu",
        "it",
        "iu",
        "av",
        "sg",
        "hz",
        "lv",
        "ts",
        "lrc",
        "ar",
        "hu",
        "nn",
        "nso",
        "krc",
        "mk",
        "tg",
        "ne",
        "dz",
        "ta",
        "mus",
        "ady",
        "en",
        "lg",
        "xal",
        "gu",
        "pt",
        "xh",
        "szl",
        "chr",
    ]

    def __init__(self, component_config: Dict[Text, Any] = None) -> None:
        """Construct a new tokenizer using the SentencePiece framework."""

        super().__init__(component_config)

        self.case_sensitive = self.component_config["case_sensitive"]
        model_file = self.component_config["model_file"]
        if model_file:
            if not os.path.exists(model_file):
                raise FileNotFoundError(
                    f"SentencePiece model {model_file} not found. Please check config."
                )
        self.model = spm.SentencePieceProcessor(model_file=model_file)

    def tokenize(self, message: Message, attribute: Text) -> List[Token]:
        text = message.get(attribute)

        if not self.case_sensitive:
            text = text.lower()
        words = self.model.encode(text, out_type=str)

        if not words:
            words = [text]

        return self._convert_words_to_tokens(words, text)
